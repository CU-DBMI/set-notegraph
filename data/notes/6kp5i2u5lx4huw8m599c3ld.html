<h1 id="install">Install<a aria-hidden="true" class="anchor-heading icon-link" href="#install"></a></h1>
<h2 id="prerequisites-and-tools">Prerequisites and Tools<a aria-hidden="true" class="anchor-heading icon-link" href="#prerequisites-and-tools"></a></h2>
<p>Assumes kubectl is already installed on your workstation.</p>
<p>Lens is recommended for watching deployment, service and pod provisioning. <a href="https://k8slens.dev/">https://k8slens.dev/</a></p>
<p>Review the knative install guide: <a href="https://knative.dev/docs/install/operator/knative-with-operators/#install-the-knative-operator">https://knative.dev/docs/install/operator/knative-with-operators/#install-the-knative-operator</a></p>
<p>Requires <code>helm</code>.</p>
<p>YAML files referenced in this doc are under \src\knative-install-resources.</p>
<p>This doc is written against knative 1.3.1.</p>
<h2 id="install-kubernetes-control-plane-and-workers">Install Kubernetes Control Plane and Workers<a aria-hidden="true" class="anchor-heading icon-link" href="#install-kubernetes-control-plane-and-workers"></a></h2>
<p>Install a zone cluster (1 per billing account is free) for MVP.</p>
<p>This is currently installed as one large cluster, but instead install the cluster, create an autoscaling worker pool, then delete the default pool. </p>
<p>TODO - refactor for the above and include reference to the terraform.</p>
<pre><code>gcloud container --project "cuhealthai-foundations" clusters create "zonal-cluster-1" --zone "us-central1-c" --no-enable-basic-auth --cluster-version "1.21.6-gke.1500" --release-channel "regular" --machine-type "e2-medium" --image-type "COS_CONTAINERD" --disk-type "pd-standard" --disk-size "100" --metadata disable-legacy-endpoints=true --scopes "https://www.googleapis.com/auth/devstorage.read_only","https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/servicecontrol","https://www.googleapis.com/auth/service.management.readonly","https://www.googleapis.com/auth/trace.append" --max-pods-per-node "110" --num-nodes "3" --logging=SYSTEM,WORKLOAD --monitoring=SYSTEM --enable-ip-alias --network "projects/cuhealthai-foundations/global/networks/default" --subnetwork "projects/cuhealthai-foundations/regions/us-central1/subnetworks/default" --no-enable-intra-node-visibility --default-max-pods-per-node "110" --no-enable-master-authorized-networks --addons HorizontalPodAutoscaling,HttpLoadBalancing,GcePersistentDiskCsiDriver --enable-autoupgrade --enable-autorepair --max-surge-upgrade 1 --max-unavailable-upgrade 0 --enable-shielded-nodes --node-locations "us-central1-c"
</code></pre>
<p>Configure .kube/config with credentials for the new cluster.</p>
<pre><code>gcloud container clusters get-credentials zonal-cluster-1 --zone us-central1-c --project cuhealthai-foundations
</code></pre>
<p>Copy the cluster config secret(s) to the password vault.</p>
<p>Once the cluster is up, fetching nodes should show something like this (example):</p>
<pre><code>$>kubectl get nodes
NAME                                             STATUS   ROLES    AGE     VERSION
gke-zonal-cluster-1-default-pool-1915e7e8-2wz5   Ready    &#x3C;none>   6m43s   v1.21.6-gke.1500
gke-zonal-cluster-1-default-pool-1915e7e8-dlht   Ready    &#x3C;none>   6m43s   v1.21.6-gke.1500
gke-zonal-cluster-1-default-pool-1915e7e8-qcq1   Ready    &#x3C;none>   6m43s   v1.21.6-gke.1500
</code></pre>
<h2 id="install-knative">Install knative<a aria-hidden="true" class="anchor-heading icon-link" href="#install-knative"></a></h2>
<p>Install the knative operator:</p>
<pre><code>kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.3.1/operator.yaml
</code></pre>
<p>Verify the installation; the deployment should be available (example):</p>
<pre><code>$>kubectl get deployment knative-operator
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
knative-operator   1/1     1            1           69s
</code></pre>
<h3 id="knative-serving">knative-serving<a aria-hidden="true" class="anchor-heading icon-link" href="#knative-serving"></a></h3>
<p>Install the knative-serving file:</p>
<pre><code>kubectl apply -f knative-serving.yaml
</code></pre>
<p>Fetch the external IP address for DNS configuration:</p>
<pre><code>kubectl --namespace knative-serving get service kourier
</code></pre>
<p>It may take a minute, but eventually the EXTERNAL-IP address will be assigned (example shown).</p>
<pre><code>NAME      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE
kourier   LoadBalancer   10.0.10.151   34.123.218.33   80:31370/TCP,443:31963/TCP   38s
</code></pre>
<p>Verify knative-serving is running:</p>
<pre><code>kubectl get KnativeServing knative-serving -n knative-serving
</code></pre>
<p>You should see READY = True.</p>
<pre><code>NAME              VERSION   READY   REASON
knative-serving   1.3.0     True
</code></pre>
<p>Given the EXTERNAL-IP from a couple steps above, configure DNS <code>*.default</code> and <code>*.center</code> to a A records. <code>default</code> and <code>center</code> map here to k8s namespaces. For every new namespace used with knative, we need to add a separate wildcard A entry.</p>
<h3 id="knative-eventing">knative-eventing<a aria-hidden="true" class="anchor-heading icon-link" href="#knative-eventing"></a></h3>
<p>Install knative-eventing:</p>
<pre><code>kubectl apply -f knative-eventing.yaml
</code></pre>
<p>There are lots of eventing sources. TBD how we'll use these.</p>
<h2 id="configure-tls">Configure TLS<a aria-hidden="true" class="anchor-heading icon-link" href="#configure-tls"></a></h2>
<p>Following this guide, and assuming the use of the HTTP-01 challenge. <a href="https://knative.dev/docs/serving/using-auto-tls/#enabling-auto-tls">https://knative.dev/docs/serving/using-auto-tls/#enabling-auto-tls</a></p>
<p>Add the cert-manager controller.</p>
<pre><code>kubectl apply --filename https://github.com/knative/net-certmanager/releases/download/knative-v1.3.0/release.yaml
</code></pre>
<p>Install cert-manager.</p>
<pre><code>helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.7.1 --set installCRDs=true
</code></pre>
<p>Install cluster issuer:</p>
<pre><code>kubectl apply -f letsencrypt-http01-issuer.yaml
</code></pre>
<p>Check to cluster issuer to see that READY = True:</p>
<pre><code>kubectl get clusterissuer letsencrypt-http01-issuer
</code></pre>
<p>Add a configmap for the certmananger:</p>
<pre><code>kubectl apply -f config-certmanager.yaml
</code></pre>
<p>Enable Auto-TLS. I'm patching below, but with Windows you may have to manually edit.</p>
<pre><code>kubectl patch configmap config-network --namespace knative-serving -p '{"data":{"auto-tls":"Enabled","autocreate-cluster-domain-claims":"true"}}'
</code></pre>
<p>Install a garbage collection policy</p>
<pre><code>kubectl apply -f gc.yaml
</code></pre>